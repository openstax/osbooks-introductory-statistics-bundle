<document xmlns="http://cnx.rice.edu/cnxml" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <title>Probability Distribution Needed for Hypothesis Testing</title>
  <metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m55608</md:content-id>
  <md:title>Probability Distribution Needed for Hypothesis Testing</md:title>
  <md:abstract/>
  <md:uuid>0ea575be-6eb9-40f8-8996-137f79686925</md:uuid>
</metadata>

<content>
  <para id="para-00001">Earlier in the course, we discussed sampling distributions. Particular distributions are associated with various types of hypothesis testing.</para>
  <para id="para-00002">The following table summarizes various hypothesis tests and corresponding probability distributions that will be used to conduct the test (based on the assumptions shown below):</para>
  <table summary=" " id="table-00001">
  <title/>
  <tgroup cols="4">
  <colspec colnum="1" colname="c1"/>
  <colspec colnum="2" colname="c2"/>
  <colspec colnum="3" colname="c3"/>
  <colspec colnum="4" colname="c4"/>
  <thead>
  <row>
  <entry align="center">Type of Hypothesis Test</entry>
  <entry align="center">Population Parameter</entry>
  <entry align="center">Estimated value (point estimate)</entry>
  <entry align="center">Probability Distribution Used</entry>
  </row>
  </thead>
  <tbody>
  <row>
  <entry>Hypothesis test for the mean, when the population standard deviation is known</entry>
  <entry>Population mean <m:math><m:mi>μ</m:mi></m:math></entry>
  <entry>Sample mean <m:math><m:mover><m:mi>x</m:mi><m:mo>¯</m:mo></m:mover></m:math></entry>
  <entry>Normal distribution, 
  <m:math><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover><m:mo>~</m:mo><m:mi>N</m:mi><m:mo>(</m:mo><m:msub><m:mi>μ</m:mi><m:mi>X</m:mi></m:msub><m:mo>,</m:mo><m:mfrac><m:msub><m:mi>σ</m:mi><m:mi>X</m:mi></m:msub><m:msqrt><m:mi>n</m:mi></m:msqrt></m:mfrac><m:mo>)</m:mo></m:math></entry>
  </row>
  <row>
  <entry>Hypothesis test for the mean, when the population standard deviation is unknown and the distribution of the sample mean is approximately normal</entry>
  <entry>Population mean <m:math><m:mi>μ</m:mi></m:math></entry>
  <entry>Sample mean <m:math><m:mover><m:mi>x</m:mi><m:mo>¯</m:mo></m:mover></m:math></entry>
  <entry>Student’s t-distribution, <m:math><m:msub><m:msub><m:mi>t</m:mi><m:mi>d</m:mi></m:msub><m:mi>f</m:mi></m:msub></m:math></entry>
  </row>
  <row>
  <entry>Hypothesis test for proportions</entry>
  <entry>Population proportion <m:math><m:mi>p</m:mi></m:math></entry>
  <entry>Sample proportion <m:math><m:mi>p</m:mi><m:mo>'</m:mo></m:math></entry>
  <entry>Normal distribution,
  <m:math><m:mi>P</m:mi><m:mo>'</m:mo><m:mo>~</m:mo><m:mi>N</m:mi><m:mo>(</m:mo><m:mi>p</m:mi><m:mo>,</m:mo><m:msqrt><m:mfrac><m:mrow><m:mi>p</m:mi><m:mo>·</m:mo><m:mi>q</m:mi></m:mrow><m:mi>n</m:mi></m:mfrac></m:msqrt><m:mo>)</m:mo></m:math></entry>
  </row>
</tbody>
  </tgroup>
  </table>
  <section id="fs-idp38268576"><title>Assumptions</title>
  <para id="para-00003">When you perform a <term class="no-emphasis" id="term-00001">hypothesis test</term> of a single population mean <emphasis effect="italics">μ</emphasis> using a normal distribution (often called a z-test), you take a <term id="term-00002">simple random sample</term> from the population. The population you are testing is <term id="term-00003">normally distributed</term>, or your sample size is sufficiently large. You know the value of the population <term id="term-00012">standard deviation</term>, which, in reality, is rarely known.</para>
  <para id="para-00004">When you perform a hypothesis test of a single population mean <emphasis effect="italics">μ</emphasis> using a <term id="term-00013">Student's t-distribution</term> (often called a <emphasis effect="italics">t</emphasis>-test), there are fundamental assumptions that need to be met in order for the test to work properly. Your data should be a simple random sample that comes from a population that is approximately normally distributed. You use the sample standard deviation to approximate the population standard deviation. (Note that if the sample size is sufficiently large, a <emphasis effect="italics">t</emphasis>-test will work even if the population is not approximately normally distributed).</para>
  <para id="para-00005">When you perform a hypothesis test of a single population proportion <emphasis effect="italics">p</emphasis>, you take a simple random sample from the population. You must meet the conditions for a <term id="term-00014">binomial distribution</term>: there are a certain number <emphasis effect="italics">n</emphasis> of independent trials, the outcomes of any trial are success or failure, and each trial has the same probability of a success <emphasis effect="italics">p</emphasis>. The shape of the binomial distribution needs to be similar to the shape of the normal distribution. To ensure this, the quantities <emphasis effect="italics">np</emphasis> and <emphasis effect="italics">nq</emphasis> must both be greater than five (<m:math><m:mi>n</m:mi><m:mi>p</m:mi><m:mo>&gt;</m:mo><m:mn>5</m:mn><m:mo> </m:mo></m:math> and <m:math><m:mi>n</m:mi><m:mi>q</m:mi><m:mo>&gt;</m:mo><m:mn>5</m:mn><m:mo> </m:mo></m:math>). Then the binomial distribution of a sample (estimated) proportion can be approximated by the normal distribution with <m:math><m:mi>μ</m:mi><m:mo>=</m:mo><m:mi>p</m:mi><m:mo> </m:mo></m:math>  and <m:math><m:mi>σ</m:mi><m:mo>=</m:mo><m:msqrt><m:mfrac><m:mrow><m:mi>p</m:mi><m:mi>q</m:mi></m:mrow><m:mi>n</m:mi></m:mfrac></m:msqrt></m:math>. Remember that <m:math><m:mi>q</m:mi><m:mo>=</m:mo><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>p</m:mi></m:math>.</para>
  </section>
  <section id="eip-162"><title>Hypothesis Test for the Mean</title><para id="eip-328">
Going back to the standardizing formula we can derive the <term id="term-00004">test statistic</term> for testing hypotheses concerning means.
</para><equation class="unnumbered" id="eip-255"><label/><m:math>
<m:msub><m:mi>Z</m:mi><m:mi>c</m:mi></m:msub>
<m:mo>=</m:mo>
<m:mfrac>
<m:mrow>
<m:mover accent="true"><m:mi>x</m:mi><m:mo>¯</m:mo></m:mover><m:mo>-</m:mo><m:msub><m:mi>μ</m:mi><m:mn>0</m:mn></m:msub></m:mrow>
<m:mrow>
<m:mi>σ</m:mi><m:mo>/</m:mo><m:msqrt><m:mi>n</m:mi></m:msqrt></m:mrow></m:mfrac>
</m:math></equation><para id="eip-37">The standardizing formula cannot be solved as it is because we do not have μ, the population mean. However, if we substitute in the hypothesized value of the mean, μ<sub>0</sub> in the formula as above, we can compute a Z value. This is the test statistic for a test of hypothesis for a mean and is presented in <link target-id="eip-id7318150"/>. We interpret this Z value as the associated probability that a sample with a sample mean of <m:math><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover></m:math>  could have come from a distribution with a population mean of <emphasis effect="italics">H<sub>0</sub></emphasis> and we call this Z value Z<sub>c</sub> for “calculated”. <link target-id="eip-id7318150"/> and <link target-id="eip-idm376525904"/> show this process.  </para><figure class="scaled-down" id="eip-id7318150"><media id="eip-id1165937423808" alt="...">

  <image mime-type="image/jpeg" src="../../media/fig-ch09_01_01n.png"/>

</media>
        </figure><para id="eip-541">In <link target-id="eip-id7318150"/> two of the three possible outcomes are presented.
<m:math><m:msub><m:mrow><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover></m:mrow><m:mn>1</m:mn></m:msub></m:math> and <m:math><m:msub><m:mrow><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover></m:mrow><m:mn>3</m:mn></m:msub></m:math> are in the tails of the hypothesized distribution of <emphasis effect="italics">H<sub>0</sub></emphasis>. Notice that the horizontal axis in the top panel is labeled <m:math><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover></m:math>'s.  This is the same theoretical distribution of <m:math><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover></m:math>'s,   the sampling distribution, that the Central Limit Theorem tells us is normally distributed. This is why we can draw it with this shape. The horizontal axis of the bottom panel is labeled Z and is the standard normal distribution. <m:math><m:msub><m:mi>Z</m:mi><m:mrow><m:mfrac><m:mrow><m:mi>α</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:mfrac></m:mrow></m:msub></m:math> and <m:math><m:msub><m:mi>-Z</m:mi><m:mrow><m:mfrac><m:mrow><m:mi>α</m:mi></m:mrow><m:mrow><m:mn>2</m:mn></m:mrow></m:mfrac></m:mrow></m:msub></m:math>, called the <term id="term-00005">critical values</term>, are marked on the bottom panel as the Z values associated with the probability the analyst has set as the level of significance in the test, (α). The probabilities in the tails of both panels are, therefore, the same.  </para><para id="eip-233">Notice that for each <m:math><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover></m:math>  there is an associated Z<sub>c</sub>, called the calculated Z, that comes from solving the equation above. This calculated Z is nothing more than the number of standard deviations that the <emphasis>hypothesized</emphasis> mean is from the sample mean. If the sample mean falls "too many" standard deviations from the hypothesized mean we conclude that the <emphasis>sample</emphasis> mean could not have come from the distribution with the hypothesized mean, given our pre-set required level of significance. It <emphasis>could</emphasis> have come from <emphasis effect="italics">H<sub>0</sub></emphasis>, but it is deemed just too unlikely. In <link target-id="eip-id7318150"/> both <m:math><m:msub><m:mrow><m:mover><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover></m:mrow><m:mn>1</m:mn></m:msub></m:math> and <m:math><m:msub><m:mrow><m:mover><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover></m:mrow><m:mn>3</m:mn></m:msub></m:math>  are in the tails of the distribution. They are deemed "too far" from the hypothesized value of the mean given the chosen level of alpha. If in fact this sample mean it did come from <emphasis effect="italics">H<sub>0</sub></emphasis>, but from in the tail, we have made a Type I error: we have rejected a good null. Our only real comfort is that we know the probability of making such an error, α, and we can control the size of α. </para><para id="eip-410"><link target-id="eip-idm376525904"/> shows the third possibility for the location of the  sample mean, <m:math><m:mover><m:mi>x</m:mi><m:mo>_</m:mo></m:mover></m:math>. Here the sample mean is within the two critical values. That is, within the probability of (1-α) and we cannot reject the null hypothesis. </para><figure class="scaled-down" id="eip-idm376525904"><media id="eip-idm342368352" alt="Normal distribution curves illustrating a two-tailed hypothesis test. Shows alpha/2 rejection regions, critical values (-Z_a/2, Z_a/2), and the test statistic (z_c2) for both sample means and Z-scores."><image mime-type="image/png" src="../../media/fig-ch09_01_09.png"/></media></figure><para id="eip-487">This gives us the decision rule for testing a hypothesis for a two-tailed test:</para><table id="eip-960" summary=". . ."><label/>
<tgroup cols="1"><thead>
  <row>
    <entry align="center">Decision rule: two-tail test</entry>

  </row>
</thead>
<tbody>
  <row>
    <entry>If <m:math><m:mo>|</m:mo><m:msub><m:mtext>Z</m:mtext><m:mi>c</m:mi></m:msub><m:mo>|</m:mo></m:math> &lt; <m:math><m:msub><m:mtext>Z</m:mtext><m:mrow><m:mfrac><m:mi>α</m:mi><m:mn>2</m:mn></m:mfrac></m:mrow></m:msub></m:math>: then do not REJECT <emphasis effect="italics">H<sub>0</sub></emphasis></entry>

  </row>
  <row>
    <entry>If  <m:math><m:mo>|</m:mo><m:msub><m:mtext>Z</m:mtext><m:mi>c</m:mi></m:msub><m:mo>|</m:mo></m:math> &gt; <m:math><m:msub><m:mtext>Z</m:mtext><m:mrow><m:mfrac><m:mi>α</m:mi><m:mn>2</m:mn></m:mfrac></m:mrow></m:msub></m:math> : then REJECT <emphasis effect="italics">H<sub>0</sub></emphasis></entry>

  </row>
</tbody>



















</tgroup>
</table><para id="eip-771">This rule will always be the same no matter what hypothesis we are testing or what formulas we are using to make the test. The only change will be to change the Z<sub>c</sub> to the appropriate symbol for the test statistic for the parameter being tested. Stating the decision rule another way: if the sample mean is unlikely to have come from the distribution with the hypothesized mean we cannot accept the null hypothesis. Here we define "unlikely" as having a probability less than alpha of occurring.</para></section>

<section id="fs-idm257750672"><title>P-Value Approach</title><para id="eip-470">An alternative decision rule can be developed by calculating the probability that a sample mean could be found that would give a test statistic larger than the test statistic found from the current sample data assuming that the null hypothesis is true. Here the notion of "likely" and "unlikely" is defined by the probability of drawing a sample with a mean from a population with the hypothesized mean that is either larger or smaller than that found in the sample data. Simply stated, the p-value approach compares the desired significance level, α, to the p-value which is the probability of drawing a sample mean further from the hypothesized value than the actual sample mean. A large <emphasis effect="italics">p</emphasis>-value calculated from the data indicates that we should not reject the <term id="term-00006">null hypothesis</term>. The smaller the <emphasis effect="italics">p</emphasis>-value, the more unlikely the outcome, and the stronger the evidence is against the null hypothesis. We would reject the null hypothesis if the evidence is strongly against it. The relationship between the decision rule of comparing the calculated test statistics, Z<sub>c</sub>, and the Critical Value, Z<sub>α</sub> , and using the <emphasis effect="italics">p</emphasis>-value can be seen in <link target-id="eip-id1169346631895"/>.</para><figure class="scaled-down" id="eip-id1169346631895"><media id="eip-id1169348081431" alt="...">

  <image mime-type="image/jpeg" src="../../media/fig-ch09_01_02n.png"/>

</media>
        </figure><para id="eip-872">The calculated value of the test statistic is Z<sub>c</sub> in this example and is marked on the bottom graph of the standard normal distribution because it is a Z value. In this case the calculated value is in the tail and thus we cannot accept the null hypothesis, the associated <m:math><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover></m:math> is just too unusually large to believe that it came from the distribution with a mean of µ<sub>0</sub> with a significance level of α.  </para><para id="eip-15">If we use the <emphasis effect="italics">p</emphasis>-value decision rule we need one more step. We need to find in the standard normal table the probability associated with the calculated test statistic, Z<sub>c</sub>. We then compare that to the α associated with our selected level of confidence.  In <link target-id="eip-id1169346631895"/> we see that the <emphasis effect="italics">p</emphasis>-value is less than α and therefore we cannot accept the null. We know that the <emphasis effect="italics">p</emphasis>-value is less than α because the area under the p-value is smaller than α/2. It is important to note that two researchers drawing randomly from the same population may find two different P-values from their samples. This occurs because the P-value is calculated as the probability in the tail beyond the sample mean assuming that the null hypothesis is correct. Because the sample means will in all likelihood be different this will create two different P-values. Nevertheless, the conclusions as to the null hypothesis should be different with only the level of probability of α.  </para><para id="eip-825">Here is a systematic way to make a decision of whether you cannot accept or cannot reject a null <emphasis>hypothesis</emphasis> if using the <term id="term-00007"><emphasis effect="italics">p</emphasis>-value</term> and a <term class="no-emphasis" id="term-00008">preset or preconceived α</term> (the "<term class="no-emphasis" id="term-00009">significance level</term>"). A preset <emphasis effect="italics">α</emphasis> is the probability of a <term id="term-00010">Type I</term> error (rejecting the null hypothesis when the null hypothesis is true). It may or may not be given to you at the beginning of the problem. In any case, the value of α is the decision of the analyst. When you make a decision to reject or not reject <emphasis effect="italics">H<sub>0</sub></emphasis>, do as follows:</para><list id="eip-526"><item>If α &gt; <emphasis effect="italics">p</emphasis>-value, cannot accept <emphasis effect="italics">H<sub>0</sub></emphasis>. The results of the sample data are significant. There is sufficient evidence to conclude that <emphasis effect="italics">H<sub>0</sub></emphasis> is an incorrect belief and that the <term id="term-00011">alternative hypothesis</term>,  <emphasis effect="italics">H<sub>a</sub></emphasis>, may be correct.</item>
<item>If α ≤ <emphasis effect="italics">p</emphasis>-value, cannot reject <emphasis effect="italics">H<sub>0</sub></emphasis>. The results of the sample data are not significant. There is not sufficient evidence to conclude that the alternative hypothesis, <emphasis effect="italics">H<sub>a</sub></emphasis>, may be correct. In this case the status quo stands.</item>
<item>When you "cannot reject <emphasis effect="italics">H<sub>0</sub></emphasis>", it does not mean that you should believe that <emphasis effect="italics">H<sub>0</sub></emphasis> is true. It simply means that the sample data have <emphasis>failed</emphasis> to provide sufficient evidence to cast serious doubt about the truthfulness of <emphasis effect="italics">H<sub>0</sub></emphasis>. Remember that the null is the status quo and it takes high probability to overthrow the status quo.  This bias in favor of the null hypothesis is what gives rise to the statement "tyranny of the status quo" when discussing hypothesis testing and the scientific method.</item>
</list><para id="eip-246">Both decision rules will result in the same decision and it is a matter of preference which one is used.</para></section><section id="eip-134"><title>One and Two-tailed Tests</title><para id="eip-803">The discussion of <link target-id="eip-id7318150"/>-<link target-id="eip-id1169346631895"/> was based on the null and alternative hypothesis presented in <link target-id="eip-id7318150"/>. This was called a two-tailed test because the alternative hypothesis allowed that the mean could have come from a population which was either larger or smaller than the hypothesized mean in the null hypothesis. This could be seen by the statement of the alternative hypothesis as μ ≠ 100, in this example.
</para><para id="eip-725">It may be that the analyst has no concern about the value being "too" high or "too" low from the hypothesized value. If this is the case, it becomes a one-tailed test and all of the alpha probability is placed in just one tail and not split into α/2 as in the above case of a two-tailed test. Any test of a claim will be a one-tailed test. For example, a car manufacturer claims that their Model 17B provides gas mileage of greater than 25 miles per gallon. The null and alternative hypothesis would be:</para><list id="eip-828" list-type="labeled-item"><item><emphasis effect="italics">H<sub>0</sub></emphasis>: µ ≤ 25</item>
<item><emphasis effect="italics">H<sub>a</sub></emphasis>: µ &gt; 25</item>
</list><para id="eip-669">The claim would be in the alternative hypothesis. The burden of proof in hypothesis testing is carried in the alternative. This is because failing to reject the null, the status quo, must be accomplished with 90 or 95 percent confidence that it cannot be maintained. Said another way, we want to have only a 5 or 10 percent probability of making a Type I error, rejecting a good null; overthrowing the status quo.</para><para id="eip-695">This is a one-tailed test and all of the alpha probability is placed in just one tail and not split into α/2 as in the above case of a two-tailed test. </para><para id="eip-344"><link target-id="eip-id1164402606956"/> shows the two possible cases and the form of the null and alternative hypothesis that give rise to them.</para>

<figure class="scaled-down" id="eip-id1164402606956"><media id="eip-id1164394376164" alt="Normal distributions illustrating one-tailed hypothesis tests. The left panel shows a right-tailed test with alpha (α) as the right rejection region. The right panel displays a left-tailed test with alpha (α) as the left rejection region.">

  <image mime-type="image/jpeg" src="../../media/fig-ch09_01_03n.png"/>

</media>
        </figure><para id="eip-757">where μ<sub>0</sub> is the hypothesized value of the population mean.</para><table id="eip-210" summary="..."><title>Test Statistics for Test of Means, Varying Sample Size, Population Standard Deviation Known or Unknown</title>
<tgroup cols="2"><thead>
  <row>
    <entry>Sample size</entry>
    <entry>Test statistic</entry>
  </row>
</thead>
<tbody>
  <row>
    <entry align="center">&lt; 30<newline/>(σ unknown)</entry>
    <entry align="center">
     <m:math>
      <m:msub><m:mi>t</m:mi><m:mi>c</m:mi></m:msub><m:mo>=</m:mo>
      <m:mfrac><m:mrow><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover>
      <m:mo>-</m:mo><m:msub><m:mi>μ</m:mi><m:mn>0</m:mn></m:msub></m:mrow>
      <m:mrow><m:mi>s</m:mi><m:mo>/</m:mo><m:msqrt><m:mi>n</m:mi></m:msqrt>
      </m:mrow></m:mfrac>
     </m:math>
    </entry>
  </row>
  <row>
    <entry align="center">&lt; 30<newline/>(σ known)</entry>
    <entry align="center">
     <m:math>
      <m:msub><m:mi>Z</m:mi><m:mi>c</m:mi></m:msub><m:mo>=</m:mo>
      <m:mfrac><m:mrow><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover>
      <m:mo>-</m:mo><m:msub><m:mi>μ</m:mi><m:mn>0</m:mn></m:msub></m:mrow>
      <m:mrow><m:mi>σ</m:mi><m:mo>/</m:mo><m:msqrt><m:mi>n</m:mi></m:msqrt>
      </m:mrow></m:mfrac>
     </m:math>
    </entry>
  </row>
  <row>
    <entry align="center">&gt; 30<newline/>(σ unknown)</entry>
    <entry align="center">
     <m:math>
      <m:msub><m:mi>Z</m:mi><m:mi>c</m:mi></m:msub><m:mo>=</m:mo>
      <m:mfrac><m:mrow><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover>
      <m:mo>-</m:mo><m:msub><m:mi>μ</m:mi><m:mn>0</m:mn></m:msub></m:mrow>
      <m:mrow><m:mi>s</m:mi><m:mo>/</m:mo><m:msqrt><m:mi>n</m:mi></m:msqrt>
      </m:mrow></m:mfrac>
     </m:math>
    </entry>
  </row>
<row>
    <entry align="center">&gt; 30<newline/>(σ known)</entry>
    <entry align="center">
     <m:math>
      <m:msub><m:mi>Z</m:mi><m:mi>c</m:mi></m:msub><m:mo>=</m:mo>
      <m:mfrac><m:mrow><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover>
      <m:mo>-</m:mo><m:msub><m:mi>μ</m:mi><m:mn>0</m:mn></m:msub></m:mrow>
      <m:mrow><m:mi>σ</m:mi><m:mo>/</m:mo><m:msqrt><m:mi>n</m:mi></m:msqrt>
      </m:mrow></m:mfrac>
     </m:math>
    </entry>
  </row>
</tbody>
</tgroup>
</table></section><section id="eip-145"><title>Effects of Sample Size on Test Statistic</title><para id="eip-848">In developing the confidence intervals for the mean from a sample, we found that most often we would not have the population standard deviation, σ. If the sample size were less than 30, we could simply substitute the point estimate for σ, the sample standard deviation, s, and use the student's <emphasis effect="italics">t</emphasis>-distribution to correct for this lack of information. </para><para id="eip-843">When testing hypotheses we are faced with this same problem and the solution is exactly the same. Namely: If the population standard deviation is unknown, and the sample size is less than 30, substitute s, the point estimate for the population standard deviation, σ, in the formula for the test statistic and use the student's <emphasis effect="italics">t</emphasis>-distribution. All the formulas and figures above are unchanged except for this substitution and changing the Z distribution to the student's <emphasis effect="italics">t</emphasis>-distribution on the graph. Remember that the student's <emphasis effect="italics">t</emphasis>-distribution can only be computed knowing the proper degrees of freedom for the problem. In this case, the degrees of freedom is computed as before with confidence intervals: df = (n-1). The calculated t-value is compared to the t-value associated with the pre-set level of confidence required in the test, t<sub>α</sub>, <sub>df</sub> found in the student's t tables. If we do not know σ, but the sample size is 30 or more, we simply substitute s for σ and use the normal distribution.
</para><para id="eip-904"><link target-id="eip-210"/> summarizes these rules.</para></section><section id="eip-242"><title>A Systematic Approach for Testing a Hypothesis</title><para id="eip-813">A systematic approach to hypothesis testing follows the following steps and in this order. This template will work for all hypotheses that you will ever test.</para><list id="eip-126"><item>Set up the null and alternative hypothesis. This is typically the hardest part of the process. Here the question being asked is reviewed. What parameter is being tested, a mean, a proportion, differences in means, etc. Is this a one-tailed test or two-tailed test?</item>
<item><para id="eip-id1169328045413">Decide the level of significance required for this particular case and determine the critical value. These can be found in the appropriate statistical table. The levels of confidence typical for businesses are 80, 90, 95, 98, and 99. However, the level of significance is a policy decision and should be based upon the risk of making a Type I error, rejecting a good null. Consider the consequences of making a Type I error. </para><para id="eip-id1169354153672">Next, on the basis of the hypotheses and sample size, select the appropriate test statistic and find the relevant critical value: Z<sub>α</sub>, t<sub>α</sub>, etc. Drawing the relevant probability distribution and marking the critical value is always big help. Be sure to match the graph with the hypothesis, especially if it is a one-tailed test.</para></item>
<item>Take a sample(s) and calculate the relevant parameters: sample mean, standard deviation, or proportion. Using the formula for the test statistic from above in step 2, now calculate the test statistic for this particular case using the parameters you have just calculated.</item>
<item>Compare the calculated test statistic and the critical value. Marking these on the graph will give a good visual picture of the situation. There are now only two situations:<list id="eip-id1169343384190" list-type="enumerated" number-style="lower-alpha"><item>The test statistic is in the tail: Cannot Accept the null, the probability that this sample mean (proportion) came from the hypothesized distribution is too small to believe that it is the real home of these sample data. </item>
<item>The test statistic is not in the tail: Cannot Reject the null, the sample data are compatible with the hypothesized population parameter.  </item></list></item>
<item>Reach a conclusion. It is best to articulate the conclusion two different ways. First a formal statistical conclusion such as “With a 5 % level of significance we cannot accept the null hypotheses that the population mean is equal to XX (units of measurement)”. The second statement of the conclusion is less formal and states the action, or lack of action, required. If the formal conclusion was that above, then the informal one might be, “The machine is broken and we need to shut it down and call for repairs”.</item>
</list><para id="eip-559">All hypotheses tested will go through this same process. The only changes are the relevant formulas and those are determined by the hypothesis required to answer the original question.</para></section><section id="eip-952" class="summary">
  <title>Chapter Review</title>
  <para id="eip-948">In order for a hypothesis test’s results to be generalized to a population, certain requirements must be satisfied. </para>
  <para id="eip-996">When testing for a single population mean:</para>
  <list id="eip-idp106869104" mark-suffix="." list-type="enumerated" number-style="arabic"><item>A Student's <emphasis effect="italics">t</emphasis>-test should be used if the data come from a simple, random sample and the population is approximately normally distributed, or the sample size is large, with an unknown standard deviation.</item>
    <item>The normal test will work if the data come from a simple, random sample and the population
      is approximately normally distributed, or the sample size is large.</item>
  </list><para id="fs-idp12045392">When testing a single population proportion use a normal test for a single population proportion if the data comes from a simple, random sample, fill the requirements for a binomial distribution, and the mean number of success and the mean number of failures satisfy the conditions: <emphasis effect="italics">np</emphasis> &gt; 5 and <emphasis effect="italics">nq</emphasis> &gt; 5 where <emphasis effect="italics">n</emphasis> is the sample size, <emphasis effect="italics">p</emphasis> is the probability of a success, and <emphasis effect="italics">q</emphasis> is the probability of a failure.</para>
</section>


  <section id="eip-772" class="formula-review"><title>Formula Review</title>
    <table id="fs-id1171148550728" summary="..."><title>Test Statistics for Test of Means, Varying Sample Size, Population Known or Unknown</title>
<tgroup cols="2"><thead>
  <row>
    <entry>Sample size</entry>
    <entry>Test statistic</entry>
  </row>
</thead>
<tbody>
  <row>
    <entry align="center">&lt; 30<newline/>(σ unknown)</entry>
    <entry align="center">
     <m:math>
      <m:msub><m:mi>t</m:mi><m:mi>c</m:mi></m:msub><m:mo>=</m:mo>
      <m:mfrac><m:mrow><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover>
      <m:mo>-</m:mo><m:msub><m:mi>μ</m:mi><m:mn>0</m:mn></m:msub></m:mrow>
      <m:mrow><m:mi>s</m:mi><m:mo>/</m:mo><m:msqrt><m:mi>n</m:mi></m:msqrt>
      </m:mrow></m:mfrac>
     </m:math>
    </entry>
  </row>
  <row>
    <entry align="center">&lt; 30<newline/>(σ known)</entry>
    <entry align="center">
     <m:math>
      <m:msub><m:mi>Z</m:mi><m:mi>c</m:mi></m:msub><m:mo>=</m:mo>
      <m:mfrac><m:mrow><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover>
      <m:mo>-</m:mo><m:msub><m:mi>μ</m:mi><m:mn>0</m:mn></m:msub></m:mrow>
      <m:mrow><m:mi>σ</m:mi><m:mo>/</m:mo><m:msqrt><m:mi>n</m:mi></m:msqrt>
      </m:mrow></m:mfrac>
     </m:math>
    </entry>
  </row>
  <row>
    <entry align="center">&gt; 30<newline/>(σ unknown)</entry>
    <entry align="center">
     <m:math>
      <m:msub><m:mi>Z</m:mi><m:mi>c</m:mi></m:msub><m:mo>=</m:mo>
      <m:mfrac><m:mrow><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover>
      <m:mo>-</m:mo><m:msub><m:mi>μ</m:mi><m:mn>0</m:mn></m:msub></m:mrow>
      <m:mrow><m:mi>s</m:mi><m:mo>/</m:mo><m:msqrt><m:mi>n</m:mi></m:msqrt>
      </m:mrow></m:mfrac>
     </m:math>
    </entry>
  </row>
<row>
    <entry align="center">&gt; 30<newline/>(σ known)</entry>
    <entry align="center">
     <m:math>
      <m:msub><m:mi>Z</m:mi><m:mi>c</m:mi></m:msub><m:mo>=</m:mo>
      <m:mfrac><m:mrow><m:mover accent="true"><m:mi>X</m:mi><m:mo>¯</m:mo></m:mover>
      <m:mo>-</m:mo><m:msub><m:mi>μ</m:mi><m:mn>0</m:mn></m:msub></m:mrow>
      <m:mrow><m:mi>σ</m:mi><m:mo>/</m:mo><m:msqrt><m:mi>n</m:mi></m:msqrt>
      </m:mrow></m:mfrac>
     </m:math>
    </entry>
  </row>
</tbody>
</tgroup>
</table></section><section id="eip-496" class="practice">
  <exercise id="eip-176"><problem id="eip-548">
  <para id="eip-434">Which two distributions can you use for hypothesis testing for this chapter?</para>
</problem>

<solution id="eip-482">
  <para id="eip-665">A normal distribution or a Student’s <emphasis effect="italics">t</emphasis>-distribution</para></solution>
</exercise><exercise id="eip-492"><problem id="eip-819">
  <para id="eip-696">Which distribution do you use when you are testing a population mean and the population standard deviation is known? Assume sample size is large. Assume a normal distribution with n ≥ 30.
  </para></problem></exercise><exercise id="eip-579">
    <problem id="eip-177">
  <para id="eip-756">Which distribution do you use when the standard deviation is not known and you are testing one population mean? Assume a normal distribution, with n ≥ 30.</para></problem>

<solution id="eip-957">
  <para id="eip-853">Use a Student’s <emphasis effect="italics">t</emphasis>-distribution</para></solution>
  </exercise>
  <exercise id="eip-692"><problem id="eip-962">
  <para id="eip-204">A population mean is 13. The sample mean is 12.8, and the sample standard deviation is two. The sample size is 20. What distribution should you use to perform a hypothesis test? Assume the underlying population is normal.</para>
</problem>  </exercise><exercise id="eip-917">
    <problem id="eip-265">
  <para id="eip-995">A population has a mean is 25 and a standard deviation of five. The sample mean is 24, and the sample size is 108. What distribution should you use to perform a hypothesis test?</para>
</problem>

<solution id="eip-953">
  <para id="eip-267">a normal distribution for a single population mean</para>
</solution>
  </exercise>
  <exercise id="eip-138"><problem id="eip-208">
  <para id="eip-449">It is thought that 42% of respondents in a taste test would prefer Brand <emphasis effect="italics">A</emphasis>. In a particular test of 100 people, 39% preferred Brand <emphasis effect="italics">A</emphasis>. What distribution should you use to perform a hypothesis test?</para>
</problem></exercise><exercise id="eip-637"><problem id="eip-74">
  <para id="eip-239">You are performing a hypothesis test of a single population mean using a Student’s <emphasis effect="italics">t</emphasis>-distribution. What must you assume about the distribution of the data?</para></problem>

<solution id="eip-520">
  <para id="eip-20">It must be approximately normally distributed.</para>
</solution>
</exercise>
  <exercise id="eip-315"><problem id="eip-505">
  <para id="eip-972">You are performing a hypothesis test of a single population mean using a Student’s <emphasis effect="italics">t</emphasis>-distribution. The data are not from a simple random sample. Can you accurately perform the hypothesis test?</para></problem></exercise><exercise id="eip-254">
    <problem id="eip-543">
      <para id="eip-372">You are performing a hypothesis test of a single population proportion. What must be true about the quantities of <emphasis effect="italics">np</emphasis> and <emphasis effect="italics">nq</emphasis>?</para>
</problem>

<solution id="eip-509">
  <para id="eip-285">They must both be greater than five.</para>
</solution>
  </exercise><exercise id="eip-354"><problem id="eip-633">
  <para id="eip-457">You are performing a hypothesis test of a single population proportion. You find out that <emphasis effect="italics">np</emphasis> is less than five. What must you do to be able to perform a valid hypothesis test?</para>
</problem></exercise><exercise id="fs-idp17688688">
    <problem id="eip-577">
      <para id="eip-129">You are performing a hypothesis test of a single population proportion. The data come from which distribution?</para>
</problem>

<solution id="eip-979">
  <para id="eip-676">binomial distribution<newline/></para>
</solution>
</exercise>

</section>

<section id="fs-idm173512608" class="free-response">
<title>Homework</title>

<exercise id="fs-idm140519760">
  <problem id="fs-idm140519632"><para id="fs-idp12923264">It is believed that Lake Tahoe Community College (LTCC) Intermediate Algebra students get less than seven hours of sleep per night, on average. A survey of 22 LTCC Intermediate Algebra students generated a mean of 7.24 hours with a standard deviation of 1.93 hours. At a level of significance of 5%, do LTCC Intermediate Algebra students get less than seven hours of sleep per night, on average? The distribution to be used for this test is <m:math>
    <m:mover accent="true">
      <m:mi>X</m:mi>
      <m:mo>–</m:mo>
    </m:mover>

  </m:math> ~ ________________</para><list id="fs-idm141858480" list-type="enumerated" number-style="lower-alpha">
      <item><m:math>
        <m:mrow>
          <m:mi>N</m:mi><m:mo stretchy="false">(</m:mo><m:mn>7.24</m:mn><m:mo>,</m:mo><m:mfrac>
            <m:mrow>
              <m:mn>1.93</m:mn>
            </m:mrow>
            <m:mrow>
              <m:msqrt>
                <m:mrow>
                  <m:mn>22</m:mn>
                </m:mrow>
              </m:msqrt>

            </m:mrow>
          </m:mfrac>
          <m:mo stretchy="false">)</m:mo>
        </m:mrow>
      </m:math></item>
      <item><m:math>
        <m:mrow>
          <m:mi>N</m:mi><m:mo stretchy="false">(</m:mo><m:mn>7.24</m:mn><m:mo>,</m:mo><m:mn>1.93</m:mn><m:mo stretchy="false">)</m:mo>
        </m:mrow>
      </m:math></item>
      <item><emphasis effect="italics">t</emphasis><sub>22</sub></item>
      <item><emphasis effect="italics">t</emphasis><sub>21</sub></item>
    </list>

  </problem>

<solution id="fs-idp8543664">
<para id="fs-idm76617584">d</para>
</solution>
</exercise>

</section>

 </content>

<glossary>
<definition id="bidist">
    <term>Binomial Distribution</term>
    <meaning id="fs-idm119055264">a discrete random variable (RV) that arises from Bernoulli trials. There are a fixed number, <emphasis effect="italics">n</emphasis>, of independent trials. “Independent” means that the result of any trial (for example, trial 1) does not affect the results of the following trials, and all trials are conducted under the same conditions. Under these circumstances the binomial RV Χ is defined as the number of successes in <emphasis effect="italics">n</emphasis> trials. The notation is: <emphasis effect="italics">X ~ B(n, p)</emphasis> <emphasis effect="italics">μ</emphasis> = <emphasis effect="italics">np</emphasis>  and the standard deviation is <m:math>
 <m:mrow>
  <m:mi>σ</m:mi><m:mo>=</m:mo><m:mo> </m:mo><m:msqrt>
   <m:mrow>
    <m:mi>n</m:mi><m:mi>p</m:mi><m:mi>q</m:mi>
   </m:mrow>
  </m:msqrt>

 </m:mrow>
    </m:math>. The probability of exactly <emphasis effect="italics">x</emphasis> successes in <emphasis effect="italics">n</emphasis> trials is <m:math>
      <m:mrow>
        <m:mi>P</m:mi><m:mo stretchy="false">(</m:mo><m:mi>X</m:mi><m:mo>=</m:mo><m:mi>x</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mrow><m:mo>(</m:mo>
          <m:mrow>
            <m:mtable>
              <m:mtr>
                <m:mtd>
                  <m:mi>n</m:mi>
                </m:mtd>
              </m:mtr>
              <m:mtr>
                <m:mtd>
                  <m:mi>x</m:mi>
                </m:mtd>
              </m:mtr>

            </m:mtable>
          </m:mrow>
          <m:mo>)</m:mo></m:mrow><m:msup>
            <m:mi>p</m:mi>
            <m:mi>x</m:mi>
          </m:msup>
        <m:msup>
          <m:mi>q</m:mi>
          <m:mrow>
            <m:mi>n</m:mi><m:mo>−</m:mo><m:mi>x</m:mi>
          </m:mrow>
        </m:msup>

      </m:mrow>
    </m:math>.
</meaning>
  </definition>


<definition id="normdist">
    <term>Normal Distribution</term>
  <meaning id="id42733014">a continuous random variable (RV) with pdf <m:math>
    <m:mrow>
      <m:mi>f</m:mi><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mo> </m:mo><m:mfrac>
        <m:mn>1</m:mn>
        <m:mrow>
          <m:mi>σ</m:mi><m:msqrt>
            <m:mrow>
              <m:mn>2</m:mn><m:mi>π</m:mi>
            </m:mrow>
          </m:msqrt>

        </m:mrow>
      </m:mfrac>
      <m:msup>
        <m:mi>e</m:mi>
        <m:mrow>
          <m:mfrac>
            <m:mrow>
              <m:mo>−</m:mo><m:msup>
                <m:mrow>
                  <m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo>−</m:mo><m:mi>μ</m:mi><m:mo stretchy="false">)</m:mo>
                </m:mrow>
                <m:mn>2</m:mn>
              </m:msup>

            </m:mrow>
            <m:mrow>
              <m:mn>2</m:mn><m:msup>
                <m:mi>σ</m:mi>
                <m:mn>2</m:mn>
              </m:msup>

            </m:mrow>
          </m:mfrac>

        </m:mrow>
      </m:msup>

    </m:mrow>
  </m:math>, where <emphasis effect="italics">μ</emphasis> is the mean of the distribution, and <emphasis effect="italics">σ</emphasis> is the standard deviation, notation: <emphasis effect="italics">X ~ N</emphasis>(<emphasis effect="italics">μ</emphasis>, <emphasis effect="italics">σ</emphasis>). If <emphasis effect="italics">μ</emphasis> = 0 and <emphasis effect="italics">σ</emphasis> = 1, the RV is called <emphasis>the standard normal distribution</emphasis>.</meaning>
  </definition>

  <definition id="stddev">
    <term>Standard Deviation</term>
    <meaning id="id20302532">a number that is equal to the square root of the variance and measures how far data values are from their mean; notation: <emphasis effect="italics">s</emphasis> for sample standard deviation and <emphasis effect="italics">σ</emphasis> for population standard deviation.</meaning>
  </definition>

<definition id="studenttdist">
    <term>Student's <emphasis effect="italics">t</emphasis>-Distribution</term>
    <meaning id="id8759760">investigated and reported by William S. Gossett in 1908 and published under the pseudonym Student. The major characteristics of the random variable (RV) are:
<list id="tdist1" list-type="bulleted">
  <item>It is continuous and assumes any real values.</item>
  <item>The pdf is symmetrical about its mean of zero. However, it is more spread out and flatter at the apex than the normal distribution.</item>
  <item>It approaches the standard normal distribution as <emphasis effect="italics">n</emphasis> gets larger.</item>
  <item>There is a "family" of <emphasis effect="italics">t</emphasis>-distributions: every representative of the family is completely defined by the number of degrees of freedom which is one less than the number of data items.</item></list></meaning>
  </definition>

<definition id="teststatistic">
<term>Test Statistic</term> <meaning id="fs-id1165813764231">The formula that counts the number of standard deviations on the relevant distribution  that estimated parameter is away from the hypothesized value.</meaning></definition>

<definition id="criticalvalue">
<term>Critical Value</term> <meaning id="fs-id1167884651205">The <emphasis effect="italics">t</emphasis> or <emphasis effect="italics">Z</emphasis> value set by the researcher that measures the probability of a Type I error, α.</meaning></definition>


</glossary>
</document>
